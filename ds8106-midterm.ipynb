{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Users/divyansh/.pyenv/versions/3.9.5/envs/.pyspark/lib/python3.9/site-packages (3.5.3)\n",
      "Requirement already satisfied: datasketch in /Users/divyansh/.pyenv/versions/3.9.5/envs/.pyspark/lib/python3.9/site-packages (1.6.5)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /Users/divyansh/.pyenv/versions/3.9.5/envs/.pyspark/lib/python3.9/site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: numpy>=1.11 in /Users/divyansh/.pyenv/versions/3.9.5/envs/.pyspark/lib/python3.9/site-packages (from datasketch) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /Users/divyansh/.pyenv/versions/3.9.5/envs/.pyspark/lib/python3.9/site-packages (from datasketch) (1.13.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/Users/divyansh/.pyenv/versions/3.9.5/envs/.pyspark/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark datasketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/10 21:28:18 WARN Utils: Your hostname, Divyanshs-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.138 instead (on interface en0)\n",
      "24/11/10 21:28:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/10 21:28:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/Users/divyansh/.pyenv/versions/3.9.5/envs/.pyspark/lib/python3.9/site-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "conf = conf.setAppName(\"ds8106-midterm\")\n",
    "# conf.set('spark.ui.proxyBase', '/user/' + os.environ['JUPYTERHUB_USER'] + '/proxy/4040') ## to setup SPARK UI\n",
    "# conf = conf.set('spark.jars', os.environ['GRAPHFRAMES_PATH']) ## graphframes in spark configuration\n",
    "\n",
    "try:\n",
    "    sc = pyspark.SparkContext(conf=conf)\n",
    "except ValueError:\n",
    "    # If a SparkContext is already running, get it\n",
    "    sc = pyspark.SparkContext.getOrCreate()\n",
    "\n",
    " # streaming representation of this variable (jp notebook thingy)\n",
    "spark = pyspark.SQLContext.getOrCreate(sc)\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+------------+-----------------------+\n",
      "|trigram           |count|bigram_count|conditional probability|\n",
      "+------------------+-----+------------+-----------------------+\n",
      "|lt p gt           |1928 |1930        |0.9989637305699481     |\n",
      "|the covid 19      |1718 |1760        |0.9761363636363637     |\n",
      "|do n t            |1662 |1662        |1.0                    |\n",
      "|of covid 19       |1589 |1621        |0.9802590993214065     |\n",
      "|the spread of     |1196 |1306        |0.9157733537519143     |\n",
      "|the number of     |1037 |1103        |0.9401631912964642     |\n",
      "|p gt lt           |1037 |1936        |0.5356404958677686     |\n",
      "|gt lt p           |1023 |1792        |0.5708705357142857     |\n",
      "|one of the        |953  |1491        |0.6391683433936955     |\n",
      "|of the coronavirus|907  |17484       |0.0518760009151224     |\n",
      "+------------------+-----+------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram, Tokenizer\n",
    "\n",
    "text_df = spark.read.text(\"data/hw1text\")\n",
    "\n",
    "processed_df = text_df.select(\n",
    "F.regexp_replace(\n",
    "        F.regexp_replace(F.lower(F.col(\"value\")), \"[^0-9a-z]\", \" \"),\n",
    "        \"\\\\s+\", \" \"\n",
    "    ).alias(\"cleaned_text\")\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"cleaned_text\", outputCol=\"words\")\n",
    "tokenized_df = tokenizer.transform(processed_df)\n",
    "\n",
    "trigram = NGram(n=3, inputCol=\"words\", outputCol=\"trigrams\")\n",
    "trigram_df = trigram.transform(tokenized_df)\n",
    "trigrams_df = trigram_df.select(F.explode(F.col(\"trigrams\")).alias(\"trigram\"))\n",
    "top_ten_trigrams = trigrams_df.groupBy(\"trigram\").count().orderBy(F.desc(\"count\")).limit(10)\n",
    "\n",
    "\n",
    "top_ten_trigrams = top_ten_trigrams.withColumn(\"first_two_words\", F.expr(\"substring_index(trigram, ' ', 2)\"))\n",
    "\n",
    "\n",
    "bigram = NGram(n=2, inputCol=\"words\", outputCol=\"bigrams\")\n",
    "bigram_df = bigram.transform(tokenized_df)\n",
    "bigrams_df = bigram_df.select(F.explode(F.col(\"bigrams\")).alias(\"bigram\"))\n",
    "bigram_counts = bigrams_df.groupBy(\"bigram\").count().withColumnRenamed(\"count\", \"bigram_count\")\n",
    "\n",
    "result_df = top_ten_trigrams.join(bigram_counts, top_ten_trigrams.first_two_words == bigram_counts.bigram, \"inner\")\n",
    "result_df = result_df.withColumn(\"conditional probability\", F.col(\"count\") / F.col(\"bigram_count\"))\n",
    "\n",
    "final_result = result_df.select(\"trigram\", \"count\", \"bigram_count\", \"conditional probability\").orderBy(F.desc(\"count\"))\n",
    "final_result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------+\n",
      "|DayPart  |Top3Items              |\n",
      "+---------+-----------------------+\n",
      "|afternoon|[Coffee, Bread, Tea]   |\n",
      "|evening  |[Coffee, Bread, Tea]   |\n",
      "|morning  |[Coffee, Bread, Pastry]|\n",
      "|noon     |[Coffee, Bread, Tea]   |\n",
      "+---------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bakery = spark\\\n",
    "  .read\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .csv(\"data/Bakery.csv\") \n",
    "\n",
    "transformedBakery = bakery \\\n",
    "    .withColumn(\"DayPart\", \n",
    "        F.when((F.hour(\"Time\") >= 6) & (F.hour(\"Time\") < 11), \"morning\")\n",
    "         .when((F.hour(\"Time\") >= 11) & (F.hour(\"Time\") < 14), \"noon\")\n",
    "         .when((F.hour(\"Time\") >= 14) & (F.hour(\"Time\") < 17), \"afternoon\")\n",
    "         .otherwise(\"evening\")\n",
    "    )\n",
    "\n",
    "transformedBakery = transformedBakery \\\n",
    "    .groupBy(\"DayPart\", \"Item\") \\\n",
    "    .agg(F.count(\"Item\").alias(\"qty\"))\n",
    "\n",
    "windowSpec = Window.partitionBy(\"DayPart\").orderBy(F.desc(\"qty\"))\n",
    "\n",
    "rankedBakery = transformedBakery.withColumn(\"Rank\", F.row_number().over(windowSpec))\n",
    "\n",
    "top3Bakery = rankedBakery.filter(F.col(\"Rank\") <= 3)\n",
    "\n",
    "finalResult = top3Bakery \\\n",
    "    .groupBy(\"DayPart\") \\\n",
    "    .agg(F.collect_list(\"Item\").alias(\"Top3Items\"))\n",
    "\n",
    "finalResult.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:===================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+----------+\n",
      "|category      |headline                                                               |link                                                                                                                                   |short_description                                                                     |similarity|\n",
      "+--------------+-----------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+----------+\n",
      "|STYLE & BEAUTY|How A Simple Short Haircut Can Make For A Dramatic Makeover (VIDEO)    |https://www.huffingtonpost.com/entry/short-haircut-makeover-video_us_5b9c5c46e4b03a1dcc7e1459                                          |She's sexy and she knows it!                                                          |0.25      |\n",
      "|PARENTS       |Mom Hilariously Nails What Getting Ready Looks Like For Mothers        |https://www.huffingtonpost.com/entry/mom-hilariously-nails-what-getting-ready-looks-like-for-mothers_us_595666f0e4b0da2c73230b40       |”Maybe she’s born with it ... Maybe she’s a tired mom who doesn’t have time for this.”|0.2421875 |\n",
      "|FOOD & DRINK  |How to Make the Perfect Chocolate Chip Cookie                          |https://www.huffingtonpost.com/entry/how-to-make-the-perfect-c_us_5b9dc24ee4b03a1dcc8c8503                                             |A cookie with the perfect combination of fat, flavor, and comfort. Who needs detox?   |0.203125  |\n",
      "|PARENTS       |The Heartbreaking Story Behind This Newborn Photo                      |https://www.huffingtonpost.com/entry/the-heartbreaking-story-behind-this-newborn-photo_us_5762b765e4b05e4be86100bc                     |A mom is honoring her daughter's father, who passed away before she was born.         |0.203125  |\n",
      "|ENTERTAINMENT |Olivia Wilde Sneakily Reveals Baby's Sex While Bashing Trump On Twitter|https://www.huffingtonpost.com/entry/olivia-wilde-sneakily-reveals-babys-sex-while-bashing-trump-on-twitter_us_57e9213fe4b0e80b1ba2e78d|She's with her (and so is her daughter).                                              |0.203125  |\n",
      "+--------------+-----------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from datasketch import MinHash\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "huffpost = spark.read.json(\"data/Huffpost.json\")\n",
    "huffpost = huffpost.filter((F.col(\"short_description\").isNotNull()) & (F.col(\"short_description\") != \"\")) \\\n",
    "    .dropDuplicates([\"link\"]) \\\n",
    "    .select(\"link\", \"headline\", \"category\", \"short_description\")\n",
    "\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'\\W+', ' ', text.lower())\n",
    "    return text.split()\n",
    "\n",
    "def compute_minhash(tokens):\n",
    "    m = MinHash(num_perm=128)\n",
    "    for token in tokens:\n",
    "        m.update(token.encode('utf8'))\n",
    "    return m\n",
    "\n",
    "base_item = \"Kitten Born With Twisted Arms And Legs Finds A Mom Who Knows She\\u2019s Perfect\"\n",
    "base_tokens = preprocess(base_item)\n",
    "base_minhash = compute_minhash(base_tokens)\n",
    "\n",
    "preprocess_udf = F.udf(preprocess, ArrayType(StringType()))\n",
    "tokenized_data = huffpost.withColumn(\"tokens\", preprocess_udf(F.col(\"short_description\")))\n",
    "\n",
    "def jaccard_similarity(m1, m2):\n",
    "    return m1.jaccard(m2)\n",
    "\n",
    "similar_items_with_scores = tokenized_data.rdd.map(lambda row: {\n",
    "    \"link\": row[\"link\"],\n",
    "    \"headline\": row[\"headline\"],\n",
    "    \"category\": row[\"category\"],\n",
    "    \"short_description\": row[\"short_description\"],\n",
    "    \"similarity\": jaccard_similarity(base_minhash, compute_minhash(row[\"tokens\"]))\n",
    "})\n",
    "\n",
    "top_5_similar_items = spark.createDataFrame(similar_items_with_scores).orderBy(\"similarity\", ascending=False).limit(5)\n",
    "\n",
    "top_5_similar_items.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
